{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import lightgbm as lgb\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(y):\n",
    "    path = '../../data/std_data/'\n",
    "    x_train_std = pd.read_pickle(path +'train/{}_x.pkl'.format(str(y))).values\n",
    "    x_test_std = pd.read_pickle(path +'test/{}_x.pkl'.format(str(y))).values\n",
    "    y_train = pd.read_pickle(path +'train/{}_y.pkl'.format(str(y))).values\n",
    "    y_test = pd.read_pickle(path +'test/{}_y.pkl'.format(str(y))).values\n",
    "    features = pd.read_pickle(path +'train/{}_x.pkl'.format(str(y))).columns\n",
    "    return x_train_std, x_test_std, y_train, y_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, auc):\n",
    "    # ROC曲線をプロット\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n",
    "    plt.legend()\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # 予測した確率全体を格納\n",
    "    probs_all_lr = np.array([])\n",
    "    probs_all_lda = np.array([])\n",
    "    probs_all_sv = np.array([])\n",
    "    probs_all_lgbm = np.array([])\n",
    "    probs_all_xgb = np.array([])\n",
    "    probs_all_cb = np.array([])\n",
    "    probs_all_mlp = np.array([])\n",
    "    \n",
    "    y_true_all = np.array([])\n",
    "    \n",
    "    for y in range(1978, 2020):\n",
    "        \n",
    "        # データの生成\n",
    "        x_train_std, x_test_std, y_train, y_test, features = load_data(y)\n",
    "        y_true_all = np.hstack((y_true_all, y_test))\n",
    "       \n",
    "        # logistic regression\n",
    "        lr = LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\",  penalty=\"l2\", C=0.0001) # ロジスティック回帰モデルのインスタンスを作成\n",
    "        lr.fit(x_train_std, y_train) # ロジスティック回帰モデルの重みを学習\n",
    "        probs_lr = lr.predict_proba(x_test_std)[:,1]\n",
    "        probs_all_lr = np.hstack((probs_all_lr, probs_lr))\n",
    "        \n",
    "        # LDA\n",
    "        lda = LDA(solver=\"eigen\", shrinkage=1).fit(x_train_std,  y_train)\n",
    "        probs_lda = lda.predict_proba(x_test_std)[:,1]\n",
    "        probs_all_lda = np.hstack((probs_all_lda, probs_lda))\n",
    "        \n",
    "        # svm\n",
    "        sv = svm.SVR(kernel=\"sigmoid\",\n",
    "                                     degree=4,\n",
    "                                     gamma=0.043502212815589775,\n",
    "                                     coef0=0.20190829020616494,\n",
    "                                     tol=0.0001,\n",
    "                                     C=0.000245786293391316,\n",
    "                                     epsilon=0.3056167642389302,\n",
    "                                    verbose=False,)\n",
    "        sv.fit(x_train_std, y_train)\n",
    "        probs_sv = sv.predict(x_test_std)\n",
    "        probs_all_sv = np.hstack((probs_all_sv, probs_sv))\n",
    "        \n",
    "        # xgb\n",
    "        xgboost = xgb.XGBRegressor(silent= True, \n",
    "                               max_depth=4,\n",
    "                               learning_rate=0.12765177534095626,\n",
    "                               n_estimators = 46,\n",
    "                               gamma=0.060805284848630535,\n",
    "                               reg_lambda=4.995675788308118,\n",
    "                               reg_alpha=2.1912254426545754,\n",
    "                               sub_sample=0.45297631180790854,\n",
    "                               scale_pos_weight=1.1672978934986058)\n",
    "        xgboost.fit(x_train_std, y_train)\n",
    "        probs_xgb = xgboost.predict(x_test_std)\n",
    "        probs_all_xgb = np.hstack((probs_all_xgb, probs_xgb))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # lgbm\n",
    "        lgbm = lgb.LGBMRegressor(\n",
    "            random_state=0,\n",
    "            verbosity=-1,\n",
    "            bagging_seed=0,\n",
    "            boost_from_average='true',\n",
    "            metric='auc',\n",
    "            bagging_freq=4,\n",
    "            min_data_in_leaf=21,\n",
    "            max_depth=13,\n",
    "            learning_rate=0.08731913651405197,\n",
    "            n_estimators=3394,\n",
    "            subsample=0.7054763057027115,\n",
    "            num_leaves=438,\n",
    "            reg_lambda=0.9377125325944119,  \n",
    "        )\n",
    "        \n",
    "        lgbm.fit(x_train_std, y_train)\n",
    "        probs_lgbm = lgbm.predict(x_test_std)\n",
    "        probs_all_lgbm = np.hstack((probs_all_lgbm, probs_lgbm))\n",
    "        \n",
    "        # catboost\n",
    "        cb = catboost.CatBoostRegressor(\n",
    "            iterations=258,\n",
    "            depth=2,\n",
    "            learning_rate=0.019083573879517587,\n",
    "            random_strength=84,\n",
    "            bagging_temperature=0.3233702745357832,\n",
    "            od_type=\"Iter\",\n",
    "            od_wait=32, \n",
    "            logging_level='Silent')\n",
    "        cb.fit(x_train_std, y_train)\n",
    "        probs_cb = cb.predict(x_test_std)\n",
    "        probs_all_cb = np.hstack((probs_all_cb, probs_cb))   \n",
    "        \n",
    "        # mlp\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(32,),\n",
    "                           activation='relu',\n",
    "                           solver='adam',\n",
    "                           alpha=4.76324733221396,\n",
    "                           batch_size='auto',\n",
    "                           learning_rate='constant', \n",
    "                           learning_rate_init=0.0012043271455668674, \n",
    "                           power_t=0.5,\n",
    "                           max_iter=1000, \n",
    "                           shuffle=True,\n",
    "                           random_state=0, \n",
    "                           tol=0.0001, \n",
    "                           verbose=False, \n",
    "                           warm_start=False, \n",
    "                           momentum=0.9,\n",
    "                           nesterovs_momentum=True, \n",
    "                           early_stopping=False, \n",
    "                           validation_fraction=0.1, \n",
    "                           beta_1=0.022158342014810775, \n",
    "                           beta_2= 0.7802116425099002,\n",
    "                           epsilon=1e-08,\n",
    "                           )\n",
    "        mlp.fit(x_train_std, y_train)\n",
    "        probs_mlp = mlp.predict(x_test_std)\n",
    "        probs_all_mlp = np.hstack((probs_all_mlp, probs_mlp))   \n",
    "    \n",
    "    auc_lr = roc_auc_score(y_true_all, probs_all_lr)\n",
    "    auc_lda = roc_auc_score(y_true_all, probs_all_lda)\n",
    "    auc_sv = roc_auc_score(y_true_all, probs_all_sv)\n",
    "    auc_xgb = roc_auc_score(y_true_all, probs_all_xgb)\n",
    "    auc_lgbm = roc_auc_score(y_true_all, probs_all_lgbm)\n",
    "    auc_cb = roc_auc_score(y_true_all, probs_all_cb)\n",
    "    auc_mlp = roc_auc_score(y_true_all, probs_all_mlp)\n",
    "    \n",
    "    all_models_probs = [probs_all_lr, probs_all_lda, probs_all_sv, probs_all_xgb, probs_all_lgbm, probs_all_cb, probs_all_mlp]\n",
    "    \n",
    "    print(\"len: {0} \".format(len(y_true_all) ))\n",
    "\n",
    "    print(\"AUC LR: \",auc_lr)\n",
    "    print(\"AUC LDA: \",auc_lda)\n",
    "    print(\"AUC svm: \",auc_sv)\n",
    "    print(\"AUC xgb: \",auc_xgb)\n",
    "    print(\"AUC lgbm: \", auc_lgbm)\n",
    "    print(\"AUC CB: \",auc_cb)\n",
    "    print(\"AUC MLP: \",auc_mlp)\n",
    "    print()\n",
    "    return y_true_all, all_models_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(trial, y_true_all, all_models_probs):\n",
    "    \n",
    "    \n",
    "    w_lr =0\n",
    "    w_lda =0\n",
    "    w_sv = trial.suggest_uniform('w_sv', 0.0, 1.0)\n",
    "    w_xgb = 0\n",
    "    w_lgbm = 0\n",
    "    w_cb =  0\n",
    "    w_mlp = 1-w_sv\n",
    "    \n",
    "    # 各モデルの重み\n",
    "    w = np.array([w_lr, w_lda, w_sv, w_xgb, w_lgbm,w_cb, w_mlp])\n",
    "#     w_norm = w/np.sum(w)\n",
    "    \n",
    "    print(\"[w_lr, w_lda, w_sv, w_xgb, w_lgbm,w_cb ] = \", w)\n",
    "#     print(\"norm = \", w_norm)\n",
    "    \n",
    "    # 各モデル結果の重み付き平均\n",
    "    probs_weighted_average = np.array([0 for i in range(len(y_true_all))], dtype='float64')\n",
    "    for probs, weight in zip(all_models_probs, w):\n",
    "        probs_weighted_average += (probs * weight)\n",
    "\n",
    "    auc = roc_auc_score(y_true_all, probs_weighted_average)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true_all, probs_weighted_average)\n",
    "    \n",
    "#     plot_roc_curve(fpr, tpr, auc)\n",
    "\n",
    "    print(\"AUC all: \",auc)\n",
    "    print()\n",
    "    return -auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    \n",
    "    y_true_all, all_models_probs = train()\n",
    "    \n",
    "    study = optuna.create_study()\n",
    "    study.optimize(lambda trial: optimize(trial, y_true_all, all_models_probs), n_trials=10000)\n",
    "    print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best value is -0.7947309101155255 with parameters: {'w_lda': 0.09921014790481464, 'w_sv': 0.4530074587471793, 'w_xgb': 0.22971392450097775, 'w_lgbm': 0.2501321890556603, 'w_mlp': 0.926253219022292}.\n",
    "\n",
    "#best value is -0.7948717948717948 with parameters: {'w_lr': 0.08360537746692473, 'w_sv': 0.7436098375618999, 'w_xgb': 0.22134975813644708, 'w_lgbm': 0.23431770706459376, 'w_mlp': 0.8118605353214242}.\n",
    "\n",
    "# best value is -0.7971259509721048 with parameters: {'w_sv': 0.8002438174499535, 'w_xgb': 0.00031518275577795086, 'w_mlp': 0.0018017698441267613}.\n",
    "\n",
    "#best value is -0.7976894899971824 with parameters: {'w_sv': 0.49087490720537474, 'w_mlp': 0.00212194974236059}.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
